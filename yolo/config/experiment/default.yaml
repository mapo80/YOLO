# =============================================================================
# YOLO Training Configuration - LightningCLI Format
# =============================================================================
#
# Usage:
#   # Training from scratch
#   python -m yolo.cli fit --config yolo/config/experiment/default.yaml
#
#   # Training with pretrained weights (auto-download)
#   python -m yolo.cli fit --config yolo/config/experiment/default.yaml --model.weight_path=true
#
#   # Override hyperparameters
#   python -m yolo.cli fit --config yolo/config/experiment/default.yaml --model.learning_rate=0.001
#
#   # Resume training from checkpoint
#   python -m yolo.cli fit --config yolo/config/experiment/default.yaml --ckpt_path=runs/yolo/last.ckpt
#
#   # Validation
#   python -m yolo.cli validate --config yolo/config/experiment/default.yaml --ckpt_path=best.ckpt
#
# =============================================================================

# -----------------------------------------------------------------------------
# Trainer Configuration
# -----------------------------------------------------------------------------
trainer:
  max_epochs: 500
  accelerator: auto
  devices: auto
  precision: 16-mixed
  gradient_clip_val: 10.0
  accumulate_grad_batches: 1
  log_every_n_steps: 50
  check_val_every_n_epoch: 1

  # Callbacks
  callbacks:
    # Save top-K models with detailed filenames
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val/mAP
        mode: max
        save_top_k: 3
        save_last: true
        save_on_train_epoch_end: false
        filename: "epoch={epoch:02d}-mAP={val/mAP:.4f}"
        auto_insert_metric_name: false

    # Save best model with fixed name for easy access
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val/mAP
        mode: max
        save_top_k: 1
        filename: "best"
        auto_insert_metric_name: false

    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/mAP
        mode: max
        patience: 50
        verbose: true
        check_on_train_epoch_end: false  # Check after validation (required for resume)

    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

    - class_path: yolo.training.callbacks.YOLOProgressBar

    - class_path: yolo.training.callbacks.EvalDashboardCallback
      init_args:
        conf_prod: 0.25        # Production confidence threshold
        show_trends: true      # Show sparkline trends
        top_n_classes: 3       # N classes for TOP/WORST

    # EMA: Exponential Moving Average of model weights
    # Improves final model accuracy by averaging weights over training
    - class_path: yolo.training.callbacks.EMACallback
      init_args:
        decay: 0.9999       # EMA decay rate (higher = more smoothing)
        tau: 2000           # Warmup steps for decay ramping
        enabled: true       # Set to false to disable EMA

  # Logger
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: runs/
        name: yolo

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
model:
  # Architecture
  model_config: v9-c
  num_classes: 80
  image_size: [640, 640]

  # Class names for metrics display (optional)
  # If not specified, class indices (0, 1, 2, ...) will be used
  # Example for custom dataset:
  #   class_names:
  #     - person
  #     - car
  #     - bicycle
  class_names: null

  # Optimizer: "sgd" (default, recommended for detection) or "adamw"
  optimizer: sgd
  learning_rate: 0.01
  momentum: 0.937              # SGD only
  weight_decay: 0.0005
  adamw_betas: [0.9, 0.999]    # AdamW only

  # Warmup
  warmup_epochs: 3
  warmup_momentum: 0.8         # SGD only
  warmup_bias_lr: 0.1

  # Loss weights
  box_loss_weight: 7.5
  cls_loss_weight: 0.5
  dfl_loss_weight: 1.5

  # Pretrained weights:
  #   null  = train from scratch
  #   true  = auto-download weights based on model_config (e.g., v9-c.pt)
  #   "path/to/weights.pt" = use specific weights file
  weight_path: null

  # NMS settings
  nms_conf_threshold: 0.25
  nms_iou_threshold: 0.65
  nms_max_detections: 300

  # Metrics plots
  # Automatically saves PR, F1, P, R curves and confusion matrix
  save_metrics_plots: true
  metrics_plots_dir: null  # null = auto (runs/<experiment>/metrics/)

  # === Learning Rate Scheduler ===
  # Options: "cosine" (default), "linear", "step", "one_cycle"
  lr_scheduler: cosine

  # Scheduler-specific parameters
  lr_min_factor: 0.01           # For cosine/linear: final_lr = initial_lr * lr_min_factor
  step_size: 30                 # For step: epochs between LR decay
  step_gamma: 0.1               # For step: LR multiplication factor at each step

  # OneCycle-specific parameters (for super-convergence)
  one_cycle_pct_start: 0.3      # Fraction of training spent in warmup
  one_cycle_div_factor: 25.0    # initial_lr = max_lr / div_factor
  one_cycle_final_div_factor: 10000.0  # final_lr = initial_lr / final_div_factor

  # === Layer Freezing (Transfer Learning) ===
  freeze_backbone: false        # Freeze backbone layers
  freeze_until_epoch: 0         # Unfreeze after this epoch (0 = always frozen)
  freeze_layers: null           # List of specific layer patterns to freeze

# -----------------------------------------------------------------------------
# Data Configuration
# -----------------------------------------------------------------------------
data:
  # Dataset paths
  root: data/coco
  train_images: train2017
  val_images: val2017
  train_ann: annotations/instances_train2017.json
  val_ann: annotations/instances_val2017.json

  # DataLoader
  batch_size: 16
  num_workers: 8
  pin_memory: true
  prefetch_factor: 4        # Batches to prefetch per worker (increase if GPU stalls)

  # === Multi-Image Augmentation ===
  # Set probability to 0.0 to disable any augmentation

  # Mosaic: combines 4 or 9 images into one
  mosaic_prob: 1.0             # 0.0 = disabled, 1.0 = always
  mosaic_9_prob: 0.0           # Probability of 9-way vs 4-way mosaic

  # MixUp: blends two images together
  mixup_prob: 0.15             # 0.0 = disabled
  mixup_alpha: 32.0            # Beta distribution parameter

  # CutMix: pastes rectangular region from another image
  cutmix_prob: 0.0             # 0.0 = disabled

  # === Single-Image Augmentation ===
  # Color augmentation (HSV)
  hsv_h: 0.015                 # Hue shift (0 = disabled)
  hsv_s: 0.7                   # Saturation (0 = disabled)
  hsv_v: 0.4                   # Value/brightness (0 = disabled)

  # Geometric augmentation (RandomPerspective)
  degrees: 0.0                 # Max rotation degrees (+/-), 0 = disabled
  translate: 0.1               # Max translation as fraction of image size
  scale: 0.9                   # Scale range (1-scale to 1+scale)
  shear: 0.0                   # Max shear degrees (+/-), 0 = disabled
  perspective: 0.0             # Perspective distortion, 0 = disabled

  # Flip augmentation
  flip_lr: 0.5                 # Horizontal flip probability
  flip_ud: 0.0                 # Vertical flip probability

  # === Training Schedule ===
  # Disable mosaic/mixup/cutmix for final N epochs (improves convergence)
  close_mosaic_epochs: 15

  # === Dataset Caching ===
  # Cache parsed labels for faster loading on subsequent runs
  cache_labels: true           # true = cache labels, false = parse every time
  cache_images: none           # "none", "ram" (keep in memory), "disk" (save as .npy)
  cache_max_memory_gb: 8.0     # Max RAM for image caching (only used if cache_images=ram)
  cache_refresh: false         # Force cache regeneration (delete and rebuild)
